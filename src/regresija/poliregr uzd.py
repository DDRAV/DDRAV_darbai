#UÅ¾duotis 1:
#Sugeneruokite duomenÅ³ rinkinÄ¯, kuriame santykis tarp
#ğ‘¥ ir  ğ‘¦ atitinka kubinÄ™ priklausomybÄ™ (pvz. y=ax3+bx2+cx+d su triukÅ¡mu). Pritaikykite 3 laipsnio polinominÄ—s regresijos modelÄ¯ Å¡iems duomenims ir vizualizuokite rezultatÄ….

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error

# Sugeneruojami kubiniai duomenys su triukÅ¡mu
np.random.seed(0)
x = np.random.rand(100, 1) * 10  # Atsitiktiniai x duomenys nuo 0 iki 10
y = 2 - 3 * x + 4 * x**2 - 0.5 * x**3 + np.random.randn(100, 1)  # KubinÄ— priklausomybÄ— su triukÅ¡mu

# DuomenÅ³ grafikas
plt.scatter(x, y, color='blue')
plt.title("3 laipsnis")
plt.xlabel("x")
plt.ylabel("y")
plt.show()

# 3 laipsnio polinominÄ— transformacija
poly = PolynomialFeatures(degree=3)
x_poly = poly.fit_transform(x)

# TiesinÄ—s regresijos modelio treniravimas naudojant polinomines ypatybes
model = LinearRegression()
model.fit(x_poly, y)

# SurÅ«Å¡iuoti duomenys sklandÅ¾iam grafiko atvaizdavimui
x_sorted = np.sort(x, axis=0)
x_poly_sorted = poly.transform(x_sorted)

# PrognozÄ—s remiantis modeliu
y_pred_sorted = model.predict(x_poly_sorted)

# RezultatÅ³ grafikas
plt.scatter(x, y, color='blue')
plt.plot(x_sorted, y_pred_sorted, color='red', linewidth=2)  # Raudona linija rodo modelio prognozÄ™
plt.title("PolinominÄ— regresija (3 laipsnio)")
plt.xlabel("x")
plt.ylabel("y")
plt.show()

# Modelio vertinimas
mse = mean_squared_error(y, model.predict(x_poly))
print(f"VidutinÄ— kvadratinÄ— klaida: {mse}")


#UÅ¾duotis 2:
#Pritaikyti ant 1 uÅ¾d. sukurtÅ³ duomenÅ³ tiesinÄ™ regresijÄ…. ApskaiÄiuokite MSE koeficientus ir palyginkite rezultatus.
linear_model = LinearRegression()
linear_model.fit(x, y)

# PrognozÄ—s naudojant tiesinÄ™ regresijÄ…
y_pred_linear = linear_model.predict(x)

# RezultatÅ³ grafikas su tiesine regresija
plt.scatter(x, y, color='blue')
plt.plot(x_sorted, linear_model.predict(x_sorted), color='green', linewidth=2)  # Å½alia linija rodo tiesinÄ™ regresijÄ…
plt.title("TiesinÄ—s regresijos rezultatas")
plt.xlabel("x")
plt.ylabel("y")
plt.show()

# MSE apskaiÄiavimas tiesinei regresijai
mse_linear = mean_squared_error(y, y_pred_linear)
print(f"TiesinÄ—s regresijos vidutinÄ— kvadratinÄ— klaida: {mse_linear}")

# Palyginimas su 3 laipsnio polinominÄ—s regresijos MSE
print(f"PolinominÄ—s regresijos (3 laipsnio) vidutinÄ— kvadratinÄ— klaida: {mse}")


#UÅ¾duotis 3:
#Naudodami tikrus duomenis (pvz., iÅ¡ sklearn bibliotekos, tokius kaip â€California housingâ€œ ar â€diabetes datasetâ€œ),
# nustatykite, ar polinominÄ— regresija geriau pritaiko duomenis nei linijinÄ— regresija. Eksperimentuokite su polinominÄ—s regresijos laipsniais 2, 3 ir 4.

from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split

# UÅ¾krauname â€Diabetesâ€œ duomenÅ³ rinkinÄ¯
data = load_diabetes()
X = data.data[:, 2:3]  # Naudosime vienÄ… kintamÄ…jÄ¯ (pvz., BMI)
y = data.target

# Padalijame duomenis Ä¯ treniravimo ir testavimo rinkinius
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# 1. TiesinÄ— regresija
linear_model = LinearRegression()
linear_model.fit(X_train, y_train)
y_pred_linear = linear_model.predict(X_test)
mse_linear = mean_squared_error(y_test, y_pred_linear)

# 2. PolinominÄ— regresija (laipsnis 2)
poly2 = PolynomialFeatures(degree=2)
X_poly2_train = poly2.fit_transform(X_train)
X_poly2_test = poly2.transform(X_test)

model_poly2 = LinearRegression()
model_poly2.fit(X_poly2_train, y_train)
y_pred_poly2 = model_poly2.predict(X_poly2_test)
mse_poly2 = mean_squared_error(y_test, y_pred_poly2)

# 3. PolinominÄ— regresija (laipsnis 3)
poly3 = PolynomialFeatures(degree=3)
X_poly3_train = poly3.fit_transform(X_train)
X_poly3_test = poly3.transform(X_test)

model_poly3 = LinearRegression()
model_poly3.fit(X_poly3_train, y_train)
y_pred_poly3 = model_poly3.predict(X_poly3_test)
mse_poly3 = mean_squared_error(y_test, y_pred_poly3)

# 4. PolinominÄ— regresija (laipsnis 4)
poly4 = PolynomialFeatures(degree=4)
X_poly4_train = poly4.fit_transform(X_train)
X_poly4_test = poly4.transform(X_test)

model_poly4 = LinearRegression()
model_poly4.fit(X_poly4_train, y_train)
y_pred_poly4 = model_poly4.predict(X_poly4_test)
mse_poly4 = mean_squared_error(y_test, y_pred_poly4)

# Spausdiname rezultatus
print(f"TiesinÄ—s regresijos MSE: {mse_linear}")
print(f"PolinominÄ—s regresijos MSE (laipsnis 2): {mse_poly2}")
print(f"PolinominÄ—s regresijos MSE (laipsnis 3): {mse_poly3}")
print(f"PolinominÄ—s regresijos MSE (laipsnis 4): {mse_poly4}")

# Grafikai
plt.scatter(X_test, y_test, color='blue', label="Tikri duomenys")
plt.plot(X_test, y_pred_linear, color='green', label="TiesinÄ— regresija")
plt.plot(X_test, y_pred_poly2, color='red', label="PolinominÄ— regresija (2 laipsnis)")
plt.plot(X_test, y_pred_poly3, color='orange', label="PolinominÄ— regresija (3 laipsnis)")
plt.plot(X_test, y_pred_poly4, color='purple', label="PolinominÄ— regresija (4 laipsnis)")
plt.title("Regresijos modeliÅ³ palyginimas")
plt.xlabel("BMI")
plt.ylabel("Diabeto progresavimas")
plt.legend()
plt.show()